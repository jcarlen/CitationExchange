---
title: "Extended Comment on 'Statistical Modelling of Citation Exchange between Statistics Journals'"
output: 
  pdf_document:
    fig_caption: yes
bibliography: citation_writeup.bib
---

### 1) Introduction:

```{r setup, cache = T, echo = F, results='hide', warning=F, echo = F, message=F}
library(latentnet)
library(ergm.count)
#load matrix

Cmatrix <- as.matrix(read.csv("~/Documents/citation/Citation_supplement/Data/cross-citation-matrix.csv", row.names = 1)) #47 x 47 #

#Self-citations are removed: The highest raw counts are from self-citations of CSDA (486) and StMed (628), which would seem to skew any analysis of importance. The authors exclude self-citations for the Stigler method: "Both the eigenfactor and the article influence score are computed over a 5-year time period, with journal self-citations removed to eliminate possible sources of manipulation."

Cmatrix.diag = Cmatrix #store a copy before removing diag
diag(Cmatrix) = rep(0,47) #shouldn't actually matter, as.network zeros out diag

#Note the use of the tranposed Cmatrix in the next line to correspond to standard i,j entry = citation FROM i to j. Original Cmatrix has i,j indicates citation from j to i.

Cnet = as.network(t(Cmatrix), directed=T, matrix.type="a", ignore.eval=F,  
       names.eval="citations") #as valued net, see \cite{krivitsky2015}

#as a binary network
Cbinet <- as.network(t(Cmatrix))

#cited/citing
cited = rowSums(Cmatrix) #citations in
citing = colSums(Cmatrix) #citations out
cite.ratio = cited/citing

#normalized
Cmatrix.norm = t(Cmatrix)/citing #entries are % of i's citations to j (column)
Cnet.norm = as.network(Cmatrix.norm, directed=T, matrix.type="a", ignore.eval=F,
            names.eval="citations")
```

```{r setup_jrss, cache = T, echo = F, results='hide', warning=F, echo = F, message=F, dependson = "setup"}
#code from JRSS-PR-SA-Dec-13-0008_supplement.R to the paper
journal.abbr <- rownames(Cmatrix)

Tmatrix <- Cmatrix + t(Cmatrix)
diag(Tmatrix) <- diag(Cmatrix.diag)

journals.cluster <- hclust(d = as.dist(1 - cor(Tmatrix)))
#plot(journals.cluster, sub = "", xlab = "")
#cutree(journals.cluster, h = 0.6)  
library(BradleyTerry2)

Cdata <- countsToBinomial(Cmatrix)
fit <- BTm(outcome = cbind(win1, win2), player1 = player1, player2 = player2, data = Cdata)

npairs <- NROW(Cdata)
njournals <- nlevels(Cdata$player1)
phi <- sum(residuals(fit, "pearson")^2) / (npairs - (njournals - 1))

## 3.1 Journal residuals
journal.res <- rep(NA, njournals)
res <- residuals(fit, type = "pearson")
coefs <- c(0, coef(fit)) # 0 is the coefficient of the first journal
for(i in 1:njournals){
    A <- which(Cdata$player1 == journal.abbr[i])
    B <- which(Cdata$player2 == journal.abbr[i])
    y <- c(res[A], -res[B])
    x <- c(-coefs[Cdata$player2[A]], -coefs[Cdata$player1[B]])
    journal.res[i] <- sum(y * x) / sqrt(phi * sum(x ^ 2))
}
names(journal.res) <- journal.abbr

library(qvcalc)
cov.matrix <- matrix(0, nrow = njournals, ncol = njournals)
cov.matrix[-1, -1] <- vcov(fit)
qse <- qvcalc(phi * cov.matrix , estimates = c(0, coef(fit)),
              labels = journal.abbr)

export.scores <- qse$qvframe$estimate
export.scores <- export.scores - mean(export.scores)
names(export.scores) <- journal.abbr

sort.id <- sort(export.scores, decreasing = TRUE, index.return = TRUE)$ix
fit.table <- data.frame(quasi = export.scores[sort.id], qse = qse$qvframe$quasiSE[sort.id])
rownames(fit.table)
rownames(fit.table)[c(1,6,20)] = c("JRSS-B", "JRSS-A", "JRSS-C")
match(rownames(fit.table),Cnet%v%"vertex.names")
fit.table2 = fit.table[order(match(rownames(fit.table),Cnet%v%"vertex.names")),]

```

In the *Discussion on 'Statistical Modelling of Citation Exchange Between Statistics Journals' by Cristiano Varin, Manuela Cattelan and David Firth*, we briefly consider a networks analysis of the statististics journal citation data discussed in the article.

The following is an extended discussion, expanding on the network models and adding visualizations and details of implementation in R.

### 2) Visualization:

Julyan Arbel writes in a blog post that he is surprised to see no "networks representation" of the citation data presented in the article by @varinetal [@arbel]. Plotting the network of journals offers a succinct summary of relationships, clustering and centrality. Of course, our interpretation of such a plot is sensitive to the plotting algorithm, parameters, and any underlying model.

The first figure in Arbel's post is a network plot prepared using the Gephi software (http://gephi.github.io/). It shows the traffic between journals via the edge weights, although for visual clarity only the top decile of weightiest edges is included. However, the distances between journals are not meaningful and are instead (I believe) optimized for appearance.

Similarly, Figure 1 below presents a thinned down version of the citation network. An edge ($i \to j$) is only visible if $j$ receives at least seven percent of the citations given by $i$. It shows that several journals are "parents" of some beneath them, but many journals are on the same level. It is evident why *Journal of the Royal Statistical Society, Series B* (JRSS-B), *Annals of Statistics* (AoS), *Biometrika* (Bka) and *Journal of the American Statistical Association* (JASA) are consistently the top four ranked journals, affirming "diffuse opinion within the statistical community" [@varinetal].

```{r network_thin, dependson = "setup", eval=T, cache=T, fig.keep='first', results = 'hide', echo=F, fig.cap = "Statistics journal citation network. Edges only pictured if receiver accounts for at least seven percent of sender's total citations sent. Vertex sizes are scaled to export scores under the Stigler model.", fig.height=6}

#from "setup":
# Cmatrix.norm = t(Cmatrix)/citing #entries are % of i's citations to j (column); rows sum to 1; 'citing' is total out for each journal

strength = .07 #only include "strong receivers", i.e. receiver accounts for >.07 = 7% of sender's citations sent [different form the plot in `compare_clustered`]
#interesting plot with strength = .1 (100 edges remain)  

Cstrong = t(Cmatrix)
Cstrong[Cmatrix.norm<strength] = 0
#rowSums(Cstrong>0)
Cnet.strong = as.network(Cstrong, directed=T, matrix.type="a", ignore.eval=F,  
       names.eval="citations")

par(mai = rep(.3,4))
plot.network(Cnet.strong, label=Cnet.strong%v%"vertex.names",
             edge.col = "light gray",
             #edge.col = gray(1 - Cnet.strong%e%"citations" / max(Cnet.strong%e%"citations")),
             label.col = 1, label.cex=.6, vertex.cex=(fit.table2$quasi+2)/3,
             vertex.border=F, arrowhead.cex=1.5) 

#### end ####
```

Figures ? and ? below provide examples of citation network plots in which distances between journals are model-based. The discussion describes the underlying models and corresponding code.

### 3) Network Modelling:

The Stigler model estimates "export scores", $u_i$, such that $c_{ij}$ is assumed binomially distributed with $E(c_{ij}) = t_{ij}\exp{(\alpha_i+\beta_j)} \text{ and } u_i = \alpha_i - \beta_i$, as in 'quasi-symmetry' formulation (4). Note that in Varin et al. the data is stored in an $47\text{ x }47$ cross-citation matrix $C$, in which $c_{ij}$ is the number of times journal $i$ is cited *by* journal $j$ excluding self-citations. $T$ is the symmetric total citation matrix, i.e. $t_{ij} = c_{ij} +c_{ji} \text{ for } i \neq j \text{ and } 0$ otherwise. To faciliate modeling in this paper I have transposed the data to fit a standard network framework such that $c_{ij}$ is the number of times journal $i$ cites journal $j$. This change is reflected in the notation below. In this notation, the 'quasi-symmetry' formulation (4) is expressed $E(c_{ij}) = t_{ij}\exp{(\alpha_j+\beta_i)} \text{ and } u_i = \alpha_i - \beta_i$.

We can place these assumptions in the context of a valued exponential random graph model (valued ERGM), where edge weights are directed citation counts. (See @krivitsky2015 and @krivitsky2012.) A direct extension of the Stigler model would retain the assumption of binomially distributed citations. However, to facilitate modelling we assume Poisson-distributed citatons with mean $c_{ij}$, thereby modelling a count instead of a proportion. We include `sender` ($\beta_i$) and `receiver` ($\alpha_i$) effects, so that our assumption is $c_{ij} \sim pois(\lambda_{ij} = exp(\alpha_j + \beta_i))$. <!--note change from varin et al notation.--> 

<!--latent.srp0 is the natural model, but for equivalence to the Stigler model I used the no intercept model, latent.srp1, below.-->

```{r latent_sr0, eval = T, results = 'hide', echo = F, dependson="setup", cache=T, warning=F}
#intercept model
latent.srp0 = ergmm(Cnet~ sender(base=0) + receiver(base=0), #base=0 OK because no edge term
  response = "citations", family="Poisson.log",
  control = control.ergmm(pilot.runs=1), seed = 123)
```

We implement this model using the `latentnet` package in R [@latentnet]. The Poisson model is used because binomial families in `latentnet` demand a constant number of trials across dyads. <!-- However, the `ergmm` helper made it seem that this would be possible. (See details in the `binomial_error` code chunk in the markdown version of this document.)--> It may seem natural to use the `ergm.count` [@ergmcount] package for weighted networks instead, but that package does not have `sender` and `receiver` terms implemented (see Section 5.2.5 of @krivitsky2012). In addition, this preliminary network model is equivalent <!--estimates the same up to constrainst/shift and method of fitting?--> to a Poisson GLM (see `glm` code chunk), but using the `latentnet` package sets up later analysis. The model formula is below, `latent.srp1`.

```{r binomial_error, eval=F, results='hide', echo=F}

test = ergmm(Cnet ~ sender(base=0) + receiver(base=0), response = "citations", family="binomial.logit", fam.par=list(trials=as.vector(Tmatrix)), control = control.ergmm(pilot.runs=1), verbose=2)

# Got "Backing off: too few acceptances." forever or
#warning()
# 1: In fam.par[["trials"]] * log1p(exp(eta)) :
#  longer object length is not a multiple of shorter object length
# I think eta is an n x n matrix
```

```{r latent_sr1, eval = T, results = 'hide', echo = c(2,3), dependson="setup", cache=T, warning=F}
#no intercept model
latent.srp1 = ergmm(Cnet ~ sender(base=0) + receiver(base=0) - 1, response = "citations",
  family="Poisson.log", control = control.ergmm(pilot.runs=1), seed = 123)
```

The Stigler export scores reported in Table 5 of Varin et al. are highly correlated ($.95$) with the corresponding estimates from this model ($\alpha_i - \beta_i$). The plot below compares rankings by the two methods, showing their similarity. Differences may be due in most part to the switch from binomial to Poisson expectations. (Code for a plot comparing scores instead of ranks is in the `latent_sr_analysis` code chunk in the markdown version of this document.)

```{r latent_sr01_analysis, eval = T, results = 'hide', echo = F, dependson= c("setup", "latent_sr0", "latent_sr1"), cache=T, fig.keep='last', fig.cap= 'Comparison of rankings by the Stigler and sender-receiver (latent.srp1) model. There is more differentiation towards the middle due to scores being more tightly clustered.'}

#correlation / plot (note use of mkl estimates)
cor(latent.srp0$mkl$beta[49:95] - latent.srp0$mkl$beta[2:48],fit.table2$quasi)
#reciever - sender
cor(latent.srp1$mkl$beta[48:94] - latent.srp1$mkl$beta[1:47], fit.table2$quasi)


##### how do the estimated scores compare to Stigler? ####
plot(fit.table2$quasi, latent.srp1$mkl$beta[48:94] - latent.srp1$mkl$beta[1:47], xlab = "Stigler", ylab = "Sender-Receiver Model") #type = 'n'
#text(labels = Cnet%v%"vertex.names", fit.table2$quasi, latent.srp1$mkl$beta[48:94] - latent.srp1$mkl$beta[1:47], cex = .6)


##### how do the estimated ranks compare to Stigler? ####
sl.raw = round(cbind(fit.table2$quasi,  latent.srp1$mkl$beta[48:94] - latent.srp1$mkl$beta[1:47]),3)
plot(sl.raw[,1],sl.raw[,2], xlab = "Stigler", ylab = "Sender-Receiver", main = "Raw Score")
sl = matrix(as.numeric(as.character(sl.raw)),47,2)
sl = sl[order(sl[,1]),]
sl[,1] = 1:47
sl = sl[order(sl[,2]),]
sl[,2] = 1:47
sl.rank = cbind(sl,sl[,2]-sl[,1])
plot(sl.rank[,1],sl.rank[,2], xlab = "Stigler", ylab = "Sender-Reciever (latent.srp1)", main = "Rank Comparison", type = "n")
text(labels = Cnet%v%"vertex.names", sl.rank[,1],sl.rank[,2], cex = .6)
abline(a = 0, b = 1, col ="red")
#### end ####

```

<!--quasi variance?-->

### 4) Equivalence to GLM:

Varin et al. point out in Section 5.1 that export scores can be estimated with standard GLM software. Indeed, the estimates of export scores in Table 5 of Varin et al. are identical to estimates from a quasibinomial generalized linear model (GLM) with logit link on proportional citation counts (differing only by a constant depending on the constraint imposed). The response is a vector of $\{\frac{c_{ij}}{t_{ij}}\}$ with zeros for diagonal elements. As in the Stigler model we must have a reference journal or zero-sum constraint. The model is implemented in the `glm` code chunk in the markdown version of this document.

```{r glm, eval = T, cache = T, echo = F, results = 'hide', dependson= c("setup", "setup_jrss", "latent_sr1"), fig.keep='none', warning=F}

Tmatrix <- Cmatrix + t(Cmatrix)

#### 1. Binomial GLM ####

#build design matrix
n = 47
x1 = matrix(0,n^2,n)
for (i in 1:n) { #sender
  for (j in 1:n) { 
    if (i != j) {
      x1[n*(i-1)+j,i]=1;
      x1[n*(i-1)+j,j]=-1;
    }
  }
}

#response
y = as.vector(t(Cmatrix))/as.vector(Tmatrix)
y[is.nan(y)]=0

#model
g1 = glm(y~x1-1,family = quasibinomial(link="logit"), weights = as.vector(Tmatrix))

# Results Identical to Sigler

cor(fit.table2$quasi, g1$coefficients, use="pairwise.complete.obs")
    # The constraint is not implemented directly so one score is NA
    # OR, as if we set last coef to 0
    cor(fit.table2$quasi, c(g1$coefficients[1:46], 0) )
    
plot(fit.table2$quasi, g1$coefficients, ylab = "glm", xlab = "Stigler") 
    #shifted by -.7 = fit.table2$quasi[47]

summary(y-predict(g1)*as.vector(Tmatrix))

# Overdispersion - no need to estimate seperately
summary(g1)$dispersion

#### 1a. Binomial GLM sender + receiver ####

#Setting it up with "sender"" and "receiver" effects in the design matrix gives basically the same results but it's superfluous, as the "sender" and "receiver" coefficents are perfectly negatively correlated (we're really fitting the differences).

#build design matrix:
x1a = matrix(0,n^2,2*n)
for (i in 1:n) { #sender
  for (j in 1:n) { 
    if (i != j) {
      x1a[n*(i-1)+j,i]=1;
      x1a[n*(i-1)+j,j+n]=1;
    }
  }
}

g1a = glm(y~x1a-1,family = quasibinomial(link="logit"), weights = as.vector(Tmatrix))

#identical to Stigler up to scaling and loss of 1 DOF (last value NA) :
cor(fit.table2$quasi, g1a$coefficients[1:47]-g1a$coefficients[48:94], use="pairwise.complete.obs") #=1

#"receiver" and "sender" coefficents perfectly negatively correlated.
cor(g1a$coefficients[1:47], g1a$coefficients[48:94], use = "complete.obs")

#### 2. Poisson GLM (compare to latent.srp1) ####

#response
y2 = as.vector(Cmatrix)

#model
g2 = glm.fit(x1a, y2, family = poisson(link="log"))

# MKL fit estimates almost equivalent to (shifted) latent.srp1 estiamtes
# Are the underlying models the same, just one fit with mcmc?
# '"glm.fit" uses iteratively reweighted least squares (IWLS)' (always same output)
# ergmm uses mcmc (output differs slightly each time)

plot(latent.srp1$mkl$beta[48:94] - latent.srp1$mkl$beta[1:47],
     g2$coefficients[48:94] - g2$coefficients[1:47])
cor(latent.srp1$mkl$beta[48:94] - latent.srp1$mkl$beta[1:47],
    g2$coefficients[48:94] - g2$coefficients[1:47], use="complete.obs") #.999
#lm(latent.srp1$mkl$beta[48:94] ~g2$coefficients[48:94]) #latent is g2 -.18
#lm(latent.srp1$mkl$beta[1:47] ~g2$coefficients[1:47]) #latent is g2 +.18
latent.srp1$mkl$beta[26] - latent.srp1$mkl$beta[26+47]
g2$coefficients[26] - g2$coefficients[26+47]
#Poisson GLM also close to Stigler:
cor(fit.table2$quasi, g2 $coefficients[48:94] - g2$coefficients[1:47] , 
    use="complete.obs")
plot(fit.table2$quasi, g2$coefficients[48:94] - g2$coefficients[1:47])

#### end ####

```

### 5) Network Model Extensions:

A benefit of the network model described above is extensibility, both theoretically and computationally.

We can make minor adjustments to the `latent.srp1` model above, such as including an intercept term for dyad-wise distributions, i.e., $\lambda_{ij} = \exp(\theta + \beta_i + \alpha_j)$. We can also make more significant changes, such as locating journals in latent space and adding cluster labels. These models and corresponding visualizations provide a deeper understanding of the landscape of journals.

### 5.1) Latent Space Model:

As a preliminary extension, consider the example of a two-dimensional latent space model using the `latentnet` package [@latentnet]. This model posits distances between journals as latent variables that affect edge weights (i.e., citation counts). Below, `latent.srp2` adds to `latent.srp1` that journals reside in two-dimensional euclidean space, i.e., $\lambda_{i,j} = \exp(\|Z_i - Z_j\| +\beta_i+\alpha_j)$ . (See formulation in Equation 4 of @krivitskyetal2009. For background see @hoff2002; @latentnet_jss; @latentnet.)

```{r latent_sr2, eval = T, results = 'hide', echo = c(1:3), cache=T, dependson="setup", warning=F}

latent.srp2 = ergmm(Cnet~euclidean(d=2) + sender(base=0) + receiver(base=0) - 1,
      response = "citations", family="Poisson.log", seed=123,
      control=ergmm.control(interval=200, sample.size=10000, burnin=100000)) 

# intercept model, also takes 1-2 hours to run
# latent.srp2a = ergmm(Cnet~euclidean(d=2) + sender(base=0) + receiver(base=0),
# response = "citations", family="Poisson.log", seed=123, 
#      control=ergmm.control(interval=200, sample.size=10000, burnin=100000)) 

# position only model. Worse fit, much higher BIC - see compare12S code chunk 
# latent.srp2b = ergmm(Cnet~euclidean(d=2), response = "citations", family="Poisson.log",
# seed=123, control=ergmm.control(interval=200, sample.size=10000, burnin=100000)) 
```

<!-- 3-D position estimates:-->
```{r latent_sr3, eval = T, results = 'hide', echo = F, cache=T, dependson="setup", warning=F}

latent.srp3 = ergmm(Cnet~euclidean(d=3) + sender(base=0) + receiver(base=0) - 1,
      response = "citations", family="Poisson.log", seed=123,
      control=ergmm.control(pilot.runs = 4, interval=200, sample.size=10000, burnin=100000)) 
```

```{r latent_sr23_analysis, eval = TRUE, results = 'hide', echo = FALSE, warning = FALSE, message = FALSE, dependson= c("setup", "setup_jrss", "latent_sr2", "latent_sr3"), cache=TRUE, fig.keep='last', fig.cap= 'Comparison of rankings by the Stigler and two-dimensional latent space (latent.srp2) model.'}

#correlation / plot (note use of mkl estimates)
cor(latent.srp2$mkl$beta[48:94] - latent.srp2$mkl$beta[1:47], fit.table2$quasi)
#reciever - sender
cor(latent.srp3$mkl$beta[48:94] - latent.srp3$mkl$beta[1:47], fit.table2$quasi)

##### how do the estimated scores compare to Stigler? ####

#2D
plot(fit.table2$quasi, latent.srp2$mkl$beta[48:94] - latent.srp2$mkl$beta[1:47], xlab = "Stigler", ylab = "2-D latent space") #type = 'n'
#text(labels = Cnet%v%"vertex.names", fit.table2$quasi, latent.srp2$mkl$beta[48:94] - latent.srp2$mkl$beta[1:47], cex = .6)

#3D
plot(fit.table2$quasi, latent.srp3$mkl$beta[48:94] - latent.srp3$mkl$beta[1:47], xlab = "Stigler", ylab = "3-D latent space") #type = 'n'
#text(labels = Cnet%v%"vertex.names", fit.table2$quasi, latent.srp3$mkl$beta[48:94] - latent.srp3$mkl$beta[1:47], cex = .6)

##### how do the estimated ranks compare to Stigler? ####

model2 =  latent.srp2

sl.raw = round(cbind(fit.table2$quasi,  model2$mkl$beta[48:94] - model2$mkl$beta[1:47]),3)
plot(sl.raw[,1],sl.raw[,2], xlab = "Stigler", ylab = "2-D latent space", main = "Raw Score")
sl = matrix(as.numeric(as.character(sl.raw)),47,2)
sl = sl[order(sl[,1]),]
sl[,1] = 1:47
sl = sl[order(sl[,2]),]
sl[,2] = 1:47
sl.rank = cbind(sl,sl[,2]-sl[,1])
plot(sl.rank[,1],sl.rank[,2], xlab = "Stigler", ylab = "2-D Latent space", main = "Rank Comparison", type = "n")
text(labels = Cnet%v%"vertex.names", sl.rank[,1],sl.rank[,2], cex = .6)
abline(a = 0, b = 1, col ="red")

#### end ####
```

Estimates of export scores from the Stigler model are very highly correlated (.99) with the corresponding estimates from this model (receiver minus sender coefficient). The rankings differ only slightly, as shown in Figure ?. (Code for a three-dimensional latent space model is in the `latent_sr3` code chunk and plotted against the Stigler model in the `latent_sr23_analysis` chunk. The correlation is greater than $.99$) 

Below (Figure ?, left) is a plot of estimated journal positions <!--mkl--> from `latent.srp2`. Node sizes are scaled to receiver minus sender coefficient. The coloring corresponds to the clustering model of Varin et al. (see their Section 3), as do the cluster labels on the right. Although there is no clustering term in our latent space model, the clustering of the authors is fairly well captured. The plot shows how the clusters fit together, and which journals are neighbors. However, we should be careful not to put too much stock in the exact positions. The right-hand plot displays the uncertainty in the positions using a sample of draws from the model. 

```{r latent_sr2_plot, eval = T, results = 'markup', dependson= c("setup", "latent_sr2", "setup_jrss", "networkstructure_thin", "latent_sr1"), fig.keep='last', echo = F, fig.height=6, fig.width=6, cache = T, warning=F, message=F, fig.cap= 'Estimated journal positions from the two-dimensional latent space model. Left: Point estimates with node size scaled to receiver minus sender coefficient. Right: Sample of positions from the model. Colouring is due to the hierarchical clustering of Varin et al. '}

####  plot ####

par(mai = c(.1,.1,.1,.1))
layout(matrix(c(1,1,1,2,2,1,1,1,2,2,1,1,1,2,2), 3, 5, byrow = TRUE), respect = T)
#vc = (latent.srp1$mkl$beta[48:94] - latent.srp1$mkl$beta[1:47])/2+1
vc2 = (latent.srp2$mkl$beta[48:94] - latent.srp2$mkl$beta[1:47])/2+1

plot(latent.srp2, labels = T, cex=.7, label.cex=.5, what = "mkl",
     plot.vars=F, vertex.col = cutree(journals.cluster, h = 0.6)+3, edge.col=0,
     print.formula=F, main = NA, vertex.border = "black", vertex.lwd = .3,
     vertex.cex = vc2, label.pos=3, suppress.axes=T,  xlab=NA, ylab=NA)
     # view some edges, for visibility edge only ifcorresponding citation count is > 20:
     # edge.col = as.numeric(Cnet%e%"citations">20)*8, 

####  cloud/uncertainty plot  ####

n = 47
N = 1000 
p = latent.srp2[["sample"]][["Z"]]
m = matrix(0,0,3)
for (i in 1:n) {
  p1 = cbind(p[,i,][,1],p[,i,][,2], rep((cutree(journals.cluster, h = 0.6)+3)[i], N))
  m = rbind(m,p1)
}

#null plot:
plot.ergmm(latent.srp2, xlab = NA, ylab = NA, vertex.col=0, edge.col=0, 
           plot.vars=F, suppress.axes=T, print.formula=F, vertex.border=0,
           xlim = c(-4,4), ylim = c(-3,5), main = NA, pie=F)

#add points
s = sample(1:(n * N * 10))
for (i in 1:(n * N)) {
  points(m[s[i],1], m[s[i],2], col = m[s[i],3], pch=".")
}

legend("topleft", col = 4:11, pch = 16, legend=c("review", "general", "theory/method", "appl./health", "computational","eco./envir.", "JSS", "StataJ"), cex=.8, box.col=0)

#### end ####
```

Figure ? gives a visual aid to the observation (see Section 7.2 of Varin et al.) that many journals are not significantly different in rank, and therefore 'grouped' rankings are more appropriate than traditional ordering. We see a periphery of mostly low-ranked journals on the right and a small cluster of leading journals around JRSS-B. Roughly, journals decrease in rank from left to right, but middle-ranked journals are dispersed widely from top to bottom. Centrality does not equate to rank or prestige, as we can see with *Biometrics (BSC)* and *Bernoulli (Bern)* on the edge of the plot.

Further, the two-dimensional latent space model (`latent.srp2`) accounts better for topical connections between journals than the sender-receiver only model (`latent.srp1`). The residuals of `latent.srp2` have range $(-26, 27)$ with standard deviation $3.7$, while  those of `latent.srp1` have range $(-128, 79)$ with standard deviation $8.7$. The largest residual of `latent.srp1` corresponds to citations from Biometrics (Bcs) to Statistics in Medicine (StMed). There is a clear topical connection between those journals that is not captured by their general tendency to import/export citations. In the `latent.srp2` model they are positioned near eachother and that residual drops to 3.

```{r prediction12, eval = T, echo = F, cache = T, dependson= c("setup", "setup_jrss", "latent_sr1", "latent_sr2"), warning=F, echo = F, message=F, fig.width = 5, results = 'hide', fig.keep='none', fig.cap= "Distribution of residuals without (left) and with (right) the latent space component added to the sender-receiver model. "}

z1 = predict.ergmm(latent.srp1); diag(z1) = rep(0,47); z1 = as.vector(z1)
z2 = predict.ergmm(latent.srp2); diag(z2) = rep(0,47); z2 = as.vector(z2)
y = as.vector(t(Cmatrix))
summary(y - z1); summary(y - z2)
sd(y - z1); sd(y - z2)
        
#largest residual
which.max(abs(as.vector(y - z1))) #320 -> 7, 38
max(abs(as.vector(y - z1)))
(y-z1)[320]; (y-z2)[320]


#### plots ####

#plot(1:2209, as.vector(t(Cmatrix)) - z1, pch = "*", main = "Residual Comparison", ylab = "Residual", xlab = "dyad") #outliers can indicate strong topical connections, where counts deviate from general pattern (e.g. highest value in the plot represents Statistics in Medicine (StMed) to Biometrics (Bcs) to StMed, Cmatrix[7,38])

#points(1:2209, as.vector(t(Cmatrix)) - z2, col = "red", pch = "*") 

#legend ("topleft", legend = c("latent.srp1 (positions not estimated)", "latent.srp2 (2-D latent space)"), pch = "*", pt.cex = 1.5, col = c(1,2), cex = .8)

temp = cbind(as.vector(t(Cmatrix)) - z1, as.vector(t(Cmatrix)) - z2)
colnames(temp) = c("latent.srp1", "latent.srp2")
boxplot(temp, main = "Residual Comparison")

#### end ####
```

<!--examine ranking effect of adding the latent space component: --> 
```{r compare12S, eval = F, echo = F, cache = T, dependson= c("setup", "setup_jrss", "latent_sr1", "latent_sr2"), warning=F, echo = F, message=F, fig.keep='first'}

#### compare scores from non-latent vs. latent space models ####
sr12 = round(cbind(latent.srp2$mkl$beta[48:94] - latent.srp2$mkl$beta[1:47],
       latent.srp1$mkl$beta[48:94] - latent.srp1$mkl$beta[1:47]),2)
rownames(sr12) = Cnet%v%"vertex.names"
sr12b = sr12
sr12b[,1] = rank(sr12b[,1])
sr12b[,2] = rank(sr12b[,2])
srdiff = sr12b[,1]-sr12b[,2]; #latent space - non latent space.

#### plot score comparison
plot(sr12, pch = NA, xlab = "Latent Space (latent.srp2)", ylab = "No Latent Space (latent.srp1)", main = "Score Comparison", cex.main = .8)
text(sr12, rownames(sr12), cex = .5)

## plot rank comparison ####
plot(sr12b, pch = NA, xlab = "Latent Space (latent.srp2)", ylab = "No Latent Space (latent.srp1)", main = "Rank Comparison", cex.main = .8)
text(sr12b, rownames(sr12b), cex = .5)
abline(0,1, lty=2, col="red")

#### compare latent space model with Stigler ####

srs2 = round(cbind(latent.srp2$mkl$beta[48:94] - latent.srp2$mkl$beta[1:47],
       fit.table2$quasi),2)
rownames(srs2) = Cnet%v%"vertex.names"
srs2b = srs2
srs2b[,1] = rank(srs2b[,1])
srs2b[,2] = rank(srs2b[,2])
srdiff = srs2b[,1]-srs2b[,2]; #latent space - non latent space.

#### plot score comparison

plot(srs2, pch = NA, xlab = "Latent Space (latent.srp2)", ylab = "Stigler Model", main = "Score Comparison", cex.main = .8)
text(srs2, rownames(sr12), cex = .5)

## plot rank comparison - JABES, EES move most (better-ranked in Stigler) ####

plot(srs2b, pch = NA, xlab = "Latent Space (latent.srp2)", ylab = "Stigler Model", main = "Rank Comparison", cex.main = .8)
text(srs2b, rownames(srs2b), cex = .5)
abline(0,1, lty=2, col="red")

#### BIC #### 

# bic.ergmm(latent.srp1, eff.obs=c("ties", "dyads", "actors")) #15742
# bic.ergmm(latent.srp2, eff.obs=c("ties", "dyads", "actors")) #10082
# bic.ergmm(latent.srp2b, eff.obs=c("ties", "dyads", "actors")) #21152

# This BIC can be (reasonably) safely used to select the number of clusters or which fixed effects to include in the model. It is not clear whether it is appropriate to use this BIC to select the dimension of latent space and whether or not to include random actor effects(http://cran.r-project.org/web/packages/latentnet/latentnet.pdf)

#### end ####

```

### 5.2) Latent Space Cluster Model:

We can further extend the model to include a clustering term for a fixed number of clusters. In addition to estimating cluster assignments this provides probabilities of cluster membership for each journal. (See @latentnet, @latentnet_jss). The resulting model reveals that divisions between clusters are soft and many journals should be thought to straddle two or more clusters.

```{r latent_srx2, eval = T, dependson= c("setup"), cache=T, results='hide', echo = F, warning=F, message=F}

#TWO clusters in 2 and 3 dimensions

latent.srp2.2 = ergmm(Cnet~euclidean(d=2, G=2) + sender(base=0) + receiver(base=0) - 1,  
  response = "citations", family="Poisson.log", control = control.ergmm(pilot.runs=4,
  interval=100, sample.size=5000, burnin=40000), seed = 123)

latent.srp3.2 = ergmm(Cnet~euclidean(d=3, G=2) + sender(base=0) + receiver(base=0) - 1,  
  response = "citations", family="Poisson.log", control = control.ergmm(pilot.runs=4,
  interval=100, sample.size=5000, burnin=40000), seed = 123)
```

```{r latent_srx3, eval = T, dependson= c("setup"), cache=T, results='hide', echo = F, warning=F, message=F}

#THREE clusters in 2 and 3 dimensions

latent.srp2.3 = ergmm(Cnet~euclidean(d=2, G=3) + sender(base=0) + receiver(base=0) - 1,  
  response = "citations", family="Poisson.log", control = control.ergmm(pilot.runs=4,
  interval=100, sample.size=5000, burnin=40000), seed = 123)

latent.srp3.3 = ergmm(Cnet~euclidean(d=3, G=3) + sender(base=0) + receiver(base=0) - 1,  
  response = "citations", family="Poisson.log", control = control.ergmm(pilot.runs=4,
  interval=100, sample.size=5000, burnin=40000), seed = 123)
```

<!-- models with more clusters (closer to authors' hierarchical 8 clusters); stops successfully fitting at 5 or 6. I do see not much value for adding more clusters in 2-D, and doesn't fit successfully in 3-D.-->

```{r latent_sr2x, eval = F, dependson= c("setup"), cache=T, results='hide', echo = F, warning=F, message=F}

latent.srp2.4 = ergmm(Cnet~euclidean(d=2, G=4) + sender(base=0) + receiver(base=0) - 1,  response = "citations", family="Poisson.log",
  seed = 123,
  control = control.ergmm(pilot.runs=4, interval=100, sample.size=5000, burnin=40000))
  #how to specify initial cluster manually?
  #, prior = ergmm.prior(Z.K = cutree(journals.cluster, h = 0.6)) ?

latent.srp2.5 = ergmm(Cnet~euclidean(d=2, G=5) + sender(base=0) + receiver(base=0) - 1,  response = "citations", family="Poisson.log",
  seed = 123,
  control = control.ergmm(pilot.runs=4, interval=100, sample.size=5000, burnin=40000))
```

Figure ? (left) shows the output of a three-cluster latent space model in three-dimensional space (`latent.srp3.3`). For visual clarity, an edge $i \to j$ is only shown if it accounts for at least seven percent of $j$'s total citations received. In comparison, the three-cluster model in two-dimensional space (`latent.srp2.3`, Figure ? top right) does not separate clusters well, as shown by the overlap in the variance circles (radii equals the square root of intracluster variance [@latentnet]). The middle-right plot of Figure ? shows the probabilistic group membership underlying the left-hand plot. Finally, the bottom-right plot uses the `latent.srp3.3` estimated positions, but colors nodes by the hiearchical clustering of Varin et al. The `latent.srp3.3` model agglomerates the hierarchical clusters into two major groups, one that is mostly applications and one that is theoretical, general and computational. The hierarchical 'review' cluster straddles the latent space clusters, reflecting its broad subject matter. (Code for a few additional models and plots is in the `latent_sr??` code chunks in the the markdown version of this document. I was unable to reliably fit a latent space model with six or more clusters for direct comparison to the hierarchical clustering.) 

```{r compare_clustered, eval=T, echo = F, dependson= c("setup", "setup_jrss", "latent_sr2", "latent_srx2", "latent_srx3"), cache=T, fig.keep='last', results='hide', fig.cap= ' ', fig.height = 6, fig.cap = "Comparison of two- and three-dimensional latent space cluster models. (Left) Edges only pictured if sender accounts for at least seven percent of receiver's total citations received."}

#CORRELATIONS HIGH THROUGHOUT

## correlation 2-D 2-cluster vs. Stigler, No Cluster ####

#2-D, 2-cluster vs. Stigler
cor(latent.srp2.2$mkl$beta[48:94] - latent.srp2.2$mkl$beta[1:47],fit.table2$quasi)
plot(latent.srp2.2$mkl$beta[48:94] - latent.srp2.2$mkl$beta[1:47],fit.table2$quasi)

#2-D 2-cluster vs. 2D No Cluster
cor(latent.srp2.2$mkl$beta[48:94] - latent.srp2.2$mkl$beta[1:47], latent.srp2$mkl$beta[48:94] - latent.srp2$mkl$beta[1:47])
plot(latent.srp2.2$mkl$beta[48:94] - latent.srp2.2$mkl$beta[1:47], latent.srp2$mkl$beta[48:94] - latent.srp2$mkl$beta[1:47])

## correlation between 3-D 2-cluster and Stigler, and Non-Cluster ####

#3-D, 2group vs. Stigler
cor(latent.srp3.2$mkl$beta[48:94] - latent.srp3.2$mkl$beta[1:47],fit.table2$quasi)
plot(latent.srp3.2$mkl$beta[48:94] - latent.srp3.2$mkl$beta[1:47],fit.table2$quasi)

#3-D 2 group vs. 2-D No Cluster
cor(latent.srp3.2$mkl$beta[48:94] - latent.srp3.2$mkl$beta[1:47], latent.srp2$mkl$beta[48:94] - latent.srp2$mkl$beta[1:47])
plot(latent.srp3.2$mkl$beta[48:94] - latent.srp3.2$mkl$beta[1:47], latent.srp2$mkl$beta[48:94] - latent.srp2$mkl$beta[1:47])

## correlation between 3-D 3-cluster and Stigler, and Non-Cluster ####

#3-D, 3 Cluster vs. Stigler
cor(latent.srp3.3$mkl$beta[48:94] - latent.srp3.3$mkl$beta[1:47],fit.table2$quasi)
plot(latent.srp3.3$mkl$beta[48:94] - latent.srp3.3$mkl$beta[1:47],fit.table2$quasi)

#3-D 3 group vs. 2-D No cluster
cor(latent.srp3.3$mkl$beta[48:94] - latent.srp3.3$mkl$beta[1:47], latent.srp2$mkl$beta[48:94] - latent.srp2$mkl$beta[1:47])
plot(latent.srp3.3$mkl$beta[48:94] - latent.srp3.3$mkl$beta[1:47], latent.srp2$mkl$beta[48:94] - latent.srp2$mkl$beta[1:47], type = "n")
text(latent.srp3.3$mkl$beta[48:94] - latent.srp3.3$mkl$beta[1:47], latent.srp2$mkl$beta[48:94] - latent.srp2$mkl$beta[1:47], labels = Cnet%v%"vertex.names", cex = .5)

## rank comparison 3-D 3-cluster vs. Stigler? ####
sl.raw = round(cbind(fit.table2$quasi, latent.srp3.3$mkl$beta[48:94] - latent.srp3.3$mkl$beta[1:47]),3)
sl = matrix(as.numeric(as.character(sl.raw)),47,2)
rownames(sl) = Cnet%v%"vertex.names"
sl = sl[order(sl[,1], decreasing = T),]
sl[,1] = 1:47
sl = sl[order(sl[,2]),]
sl[,2] = 47:1
sl = sl[47:1,]
sl.rank = cbind(sl,sl[,2]-sl[,1])
plot(sl.rank[,1], sl.rank[,2], xlab = "Stigler", ylab = "Latent 3-D, 3 Group", main = "Rank", type="n")
text(sl.rank[,1], sl.rank[,2], pos = 1, label = rownames(sl.rank), cex=.5)

#biggest change:
sl.rank[,1]-sl.rank[,2] #StNee -> 6 worse-ranked in cluster model

## rank comparison 3-D 3-cluster vs. 2-D no cluster? ####
sl.raw = round(cbind(latent.srp2$mkl$beta[48:94] - latent.srp2$mkl$beta[1:47], latent.srp3.3$mkl$beta[48:94] - latent.srp3.3$mkl$beta[1:47]),3)
sl = matrix(as.numeric(as.character(sl.raw)),47,2)
rownames(sl) = Cnet%v%"vertex.names"
sl = sl[order(sl[,1], decreasing = T),]
sl[,1] = 1:47
sl = sl[order(sl[,2]),]
sl[,2] = 47:1
sl = sl[47:1,]
sl.rank = cbind(sl,sl[,2]-sl[,1])
#biggest change:
sl.rank[,1]-sl.rank[,2] #EES - 9 worse in cluster model

###### plot to show - compare change in rank to clustering ####

par(mai = c(.1,.1,.1,.1))
layout(matrix(c(1,1,1,2,2,1,1,1,3,3,1,1,1,3,3,1,1,1,4,4), 4, 5, byrow = TRUE), respect = T)

#plot(sl.rank[,1], sl.rank[,2], xlab = "2-D, No cluster", ylab = "Latent 3-D, 3 Cluster", main = "Rank", type="n")
#text(sl.rank[,1], sl.rank[,2], pos = 1, label = rownames(sl.rank), cex=.5)

#only show > 10%
strength2 = .07 #only include "strong senders" [different from plot in "network_thin"], i.e. sender accounts for >.07 = 7% of receivers citations in
Cmatrix.norm2 = t(Cmatrix/rowSums(Cmatrix)) #entries are i's to j as percentage of total into j
Cnet.norm2 = as.network(Cmatrix.norm2, directed=T, matrix.type="a",
            ignore.eval=F, names.eval="citations")
col2 = Cnet.norm2%e%"citations"
col2[col2 < strength2] = 0
col2 = gray(1-col2, alpha = 1)
col2[col2=="#FFFFFFFF"]=NA

plot(latent.srp3.3, pie=F, labels=T, edge.col=col2, label.cex=.5,
     suppress.axes=T, print.formula=F, vertex.cex=2, vertex.border = NA,
     suppress.center = T)

plot(latent.srp2.3, pie=F, labels=F, edge.col=0, label.cex=.5, vertex.border = NA,
     suppress.axes = T, print.formula=F, vertex.cex=2.5, xlab = NA, ylab = NA,
     suppress.center = T)

#plot(latent.srp3.3, pie=F, labels=T, edge.col=0, label.cex=.5,
#    suppress.axes=T, print.formula=F, vertex.cex=2)

#pie
plot(latent.srp3.3, pie=T, labels=F, edge.col=0, label.cex=.5, vertex.border = NA,
     suppress.axes=T, print.formula=F, vertex.cex=2, suppress.center = T)

# vs. paper's hierarchical clustering
plot(latent.srp3.3, pie=F, labels = F, cex=.7, vertex.cex=3, plot.vars=F, vertex.col = cutree(journals.cluster, h = 0.6)+3 , vertex.border = NA, edge.col=0 ,  label.cex=.7, main = "latent.srp3.3, Hierarchical Cluster Color", suppress.axes=T, xlab=NA, ylab=NA, print.formula = F, plot.means = F, suppress.center = T)

legend("topleft", col = 4:11, pch = 16, legend=c("review", "general", "theory/method", "appl./health", "computational","eco./envir.", "JSS", "StataJ"), cex=.7, box.col=0)

## Multipe model comparison plot (Stilger, 2-D No Cluster, 2-D 2 Cluster, 3-D 3 Cluster) ####
u = cbind(fit.table2$quasi, latent.srp2$mkl$beta[48:94] - latent.srp2$mkl$beta[1:47], latent.srp2.2$mkl$beta[48:94] - latent.srp2.2$mkl$beta[1:47], latent.srp3.3$mkl$beta[48:94] - latent.srp3.3$mkl$beta[1:47])
rownames(u) = Cnet%v%"vertex.names"
colnames(u) = c("Stigler", "latent.srp2", "latent.srp2.2", "latent.srp3.3")
u = u[order(u[,1],decreasing = T),] #raw number chart
#plot(1:47, u[47:1,1], type="l", ylab = "score", xlab = "rank")
#points(1:47, u[47:1,2], type="l", col="red")
#points(1:47, u[47:1,3], type="l", col="blue")
#points(1:47, u[47:1,4], type="l", col="green")
#legend("topleft", legend = c( "Stigler", "2-D", "2-D 2 Cluster", "3-D 3 Cluster"), lty=1, col = c(1,2,"blue"))

##### end #####
```

The scores and ranks calculated from the cluster models are very similar to earlier models. (Code to compare models is in the `compare_clustered` code chunk in the markdown version of this document.) Only a few journal ranks change by more than a few places. We reiterate that ranking differences may not be significant, but reference them for ease of comparison.

### 6) Network Structure:

The Stigler model offers a method to test for significant difference in the export scores of two journals, but does not give an overall picture of network hierarchy or structure. Along with visualization, descriptive network statistics help to analyze network structure.

```{r triads, eval=T, cache=T, echo=F, results='asis'}
cat('\n![Triad types](/Users/jac/Downloads/triads.png)\n')
#cat('\n![This is myfile_1.png](/Users/jac/Downloads/triads2.png)\n')
#triad types: http://www.paulmichaelcohen.com/wp-content/uploads/2012/08/1-s2.0-S0378873301000351-gr1.jpg
# or see ?triad.classify
```

The dispersed hierarchy of statistics journals is reflected in the triad census. In the thinned network of Figure 1, 021U triads make up the majority of triads other than the single-edge type, 012, which suggests hierarchy and overall sparsity. However, there are also significant counts of 102 and 111D type triads, indicating more lateral connections. (See Figure ? for triad labels.) There are no cyclic triads of type O30C and 120C, which would indicate a lack of hieracrchy, and very few of the full (300) and near-full (210) types which also contain cycles.

In contrast, in the full network the most popular triads are by far the full (300) or almost-full (210) types, indicative of the network density. The R output below shows all network triad statistics for the thinned down (`Cnet.strong`) and full network (`Cnet`).

```{r network_census, dependson = c("setup", "network_thin"), eval=T, cache=T, echo=c(1,2), results='markup'}
summary.statistics(Cnet.strong~triadcensus())
summary.statistics(Cnet~triadcensus())
#summary.statistics(Cnet.strong~transitive()+intransitive)
#why don't the numbers sum as expected?
#transitive triads are those of type 120D, 030T, 120U, or 300
#88(2)+77+7(2)+36 #multiples account for symmetry but still off, supposed to be 318, 15 short
#intransitive triads are those of type 111D, 201, 111U, 021C, or 030C
#189+3(2)+5+92 #multiples account for symmetry but still off, supposed to be 297, 5 short
```



### 7) Conclusions:

$\pagebreak$

<!--Notes:-->

<!--test for significant difference between two scores for this model? -->

<!--Modelling with structural elements? better than a triad census? We can include structural features in a network model. We can add `transitive` and `intransitive` terms to a model on the thinned down citation network. These tally the transitive and intransitive triads respectively (see @ergm). We use the thinned network because the transitivity terms do not reflect the edge weights (citation counts) in their calculations. The estimated coefficients are very small.-->

#  References

